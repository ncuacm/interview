+ 简介：kafka是一个分布式的流式处理平台，流式平台的三个关键功能，消息队列，容错的持久化方式和存储记录消息流，以及流式处理平台。
+ consumer是推还是拉？
  + 考虑到的是consumer从broker里面拉取消息还是brokers将消息推送到consumer，也就是pull和push的区别。大多数系统都是采用消费者拉取消息的方法，但是有少数的还是采用的是brokers向consumer推送消息的方法。但是这种方法我们需要考虑到的是假如brokers推送的消息的速度远大于consumer消费的速度，那么consumer就很容易崩溃，所以，推荐还是使用consumer推送的方法。
+ 讲一讲kafka维护消息状态跟踪的方法？
  + 大部分消息系统在broker端维护消息被消费的记录：一个消息被分发到consumer之后broker就会立马进行标记或者等到consumer的通知之后进行标记。这样也可以在消费者消费之后立马删除以减少空间占用。
  + 但是这里需要考虑到一种情况，假如我们的消息在发送出去之后，consumer处理失败了，那么这条消息就丢失了。所以现在许多消息系统提供了另一个功能。当消息发送出去之后仅仅被标记为已发送状态。当接受到consumer的已经消费成功的通知之后才标记已经被消费的状态。这虽然解决了消息丢失的问题，但是又产生了一个新的问题，如果consumer处理消息成功之后但是又向broker发送响应失败了，那么broker会再次将这条消息发送给consumer，这样consumer就消费了两次这条消息，同时，broker必须维护每条消息的状态，并且每次都要先锁住消息然后更改状态然后释放锁，一方面数据量大的时候不好维护，同时加入一直没有收到consumer的回复，那么这个锁就会一直保存。
  + kafka采用了不同的策略，topic被分为若干个分区，然后每个分区在同一个时间内只能被一个consumer消费，这就意味着每个分区被消费的消息在日志中的位置仅仅是一个整数，这样，我们标记每个分区的消费状态就很容易了，仅仅需要一个整数而已。这样消费状态的跟踪就非常简单了。这样会带来另一个好处，consumer可以把offset调整成一个较老的值，去重新消费老的消息，这对传统的消息系统来说看上去不可思议，但是确实有用。
+ 为什么需要消息系统？
  + 解耦，允许你独立的扩展或者修改两边的处理过程，只要确保他们遵守同样的接口约束。
  + 冗余，消息队列把数据进行持久话知道他们已经被完全处理，通过这个方式可以规避数据被丢弃的风险，许多的消息队列所采用的是"插入-获取-删除"的范式，在把一个消息从队列中删除之前，需要你的处理系统明确的指出消息已经被处理完毕，从而确保你的数据被安全的保存知道使用完毕。
  + 扩展性，因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只需要增加另外的处理过程即可。
  + 灵活性&峰值处理能力。
  + 可恢复性：系统的一部分组件失效的时候，不会影响整个系统，消息队列降低了进程见的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统回复后被处理。
  + 顺序保证：在大多数场景中，数据处理的顺序都很重要，大部分消息队列本来就是排序的，并且能保证会按照特定的顺序来处理。
  + 缓冲，有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一直的情况。
  + 异步通信：很多使用，用户并不想立刻处理消息，消息队列提供了异步处理机制，只允许把一个消息放入队列，但是并不处理他，想向队列中放入多少消息就放多少，然后需要的时候再进行处理。
+ kafka和传统的MQ消息系统之间有三个关键的区别
  + kafka的日志系统，这些日志可以被重复读取和无期限保留
  + kafka是一个分布式系统:他以集群的方法来运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性。
  + kafka支持实时的流式处理。
+ kafka如何保证生产者的消息不丢失
  + 为了确保生产者成功发送了消息，我们需要判断发送的结果，生产者使用了send方法发送消息是一个异步的操作，但是我们可以通过get方法获取调用的结果，这样就可以变为同步的操作了。
+ kafka如何保证消费者不丢失消息
  + 消息在追加到分区的时候会分配一个特定的偏移量，表示当前消费者消费到的分区的位置，kafka通过偏移量可以保证消息在分区内的顺序性。
+ kafka为什么很快？

  + 顺序读写，kafka是将消息持久到本地磁盘的，但是不同与随机读写，快或者慢关键在于寻址的方式，kafka的寻址方式是采用的顺序读取的方法。kefka是将message追加到每个文件的末尾，我们知道每个分区是一个文件，收到消息的kafka会将数据追加到文件的末尾。但是有一个缺点就是kafka是不会删除消息的，他是通过一个offset来表示读取到了第几条消息。当然，这样的话我们的分区会越来越大，所以我们需要做的是删除一些历史的数据，删除的策略主要有定时删除和基于分区的文件大小来进行删除。
  + page cache，kafka利用的是操作系统本身的page cache，而不是使用程序自身的空间内存，这样做的操作就是避免了堆内存的消耗和避免了GC的问题。
  + 零拷贝机制，page cache结合sendfile的方法，将数据从page cache层先发送到网络，也就是一个socket缓冲区，然后就是发送到NIC缓冲区里面。
  + 分区分段+索引，kafka的message是按照topic分类进行存储的，topic的数据有是按照一个一个的分区存储到不同的存储节点上面。每个partition对应了操作系统上的一个文件夹，partition又是按照segment分段存储的，这也非常符合分布式系统的分区分桶的方法。
  + 批量读写，kafka的数据的读写是批量的，除了利用底层的技术之外，kafka还在应用层层面提供了一些手段来提升性能，最明显的就是使用批量传输。
  + 批量压缩，在很多情况下，对数据进行压缩之后可以消耗少量的CPU资源。而kafka的压缩是批量对所有的消息进行压缩，减少网络IO的消耗，通过mmap提高速度IO，也就是kafka在写的时候是通过顺序读写的操作来达到最优，在读的时候通过sendfile直接暴力输出。
+ kafka的多副本机制有哪些好处？
  + kafka的分区采用了多副本机制，分区中的多副本之间会有一个叫做leader的家伙，其他副本称为follow，消息会发送到leader副本上面，然后follower副本才可以从leader副本里吗拉取消息进行同步。简单来说就是生产者和消费者都是只和leader节点进行交互，然后其他节点都是leader节点的副本罢了。